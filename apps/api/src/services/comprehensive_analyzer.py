"""
ComprehensiveAnalyzer - Unified analysis engine
Replaces scattered nlp.py functions with batched, efficient analysis
"""

from __future__ import annotations
import asyncio
import json
import hashlib
from typing import Dict, Any, List, Optional, Tuple, Union
from dataclasses import dataclass
from enum import Enum

from src.services.llm_client import get_llm_client, LLMRequest, generate_json


class AnalysisType(str, Enum):
    HR_CRITERIA = "hr_criteria"
    JOB_FIT = "job_fit"
    HIRING_DECISION = "hiring_decision"
    CANDIDATE_PROFILE = "candidate_profile"
    SOFT_SKILLS = "soft_skills"
    REQUIREMENTS_EXTRACTION = "requirements_extraction"


@dataclass
class AnalysisInput:
    """Input data for comprehensive analysis"""
    job_description: str = ""
    transcript_text: str = ""
    resume_text: str = ""
    candidate_name: str = ""
    job_title: str = ""
    analysis_types: Optional[List[AnalysisType]] = None
    
    def __post_init__(self):
        if self.analysis_types is None:
            self.analysis_types = [
                AnalysisType.HR_CRITERIA,
                AnalysisType.JOB_FIT,
                AnalysisType.HIRING_DECISION
            ]


class ComprehensiveAnalyzer:
    """
    Unified analyzer that replaces scattered nlp.py functions
    Uses batched LLM calls for efficiency
    """
    
    def __init__(self):
        self.llm_client = get_llm_client()
    
    # --- Heuristic transcript metrics and objective penalties ---
    def _extract_candidate_utterances(self, transcript: str) -> List[str]:
        """Extract candidate utterances from transcript built as "Interviewer:"/"Candidate:" lines.
        Falls back to treating entire transcript as candidate if markers not found.
        """
        if not transcript:
            return []
        lines = [l.strip() for l in transcript.splitlines() if l.strip()]
        out: List[str] = []
        for line in lines:
            # Turkish and English markers
            if line.lower().startswith("candidate:") or line.lower().startswith("aday:"):
                out.append(line.split(":", 1)[1].strip() if ":" in line else line)
        if not out:
            # Fallback: try to split paragraphs and assume alternate turns
            parts = [p.strip() for p in transcript.split("\n\n") if p.strip()]
            # take every second part as candidate (best-effort)
            out = [p for i, p in enumerate(parts) if i % 2 == 1]
        return out

    def _compute_transcript_metrics(self, transcript: str) -> Dict[str, float]:
        """Compute simple robustness metrics that correlate with poor answers.
        - avg_answer_len_words
        - short_answer_ratio (answers <= 6 words)
        - filler_per_100_words
        - negative_phrase_count
        """
        import re
        answers = self._extract_candidate_utterances(transcript)
        if not answers:
            return {
                "avg_answer_len_words": 0.0,
                "short_answer_ratio": 1.0,
                "filler_per_100_words": 0.0,
                "negative_phrase_count": 0.0,
            }
        # Normalize text
        total_words = 0
        short = 0
        filler_words = ["ÅŸey", "hani", "yani", "Ä±Ä±", "ee", "falan", "iÅŸte"]
        negative_phrases = [
            "bilmiyorum", "emin deÄŸilim", "hiÃ§", "yok", "yapmadÄ±m", "deneyimim yok",
            "hatÄ±rlamÄ±yorum", "zorlandÄ±m", "beceremedim", "iptal", "baÅŸaramadÄ±m",
        ]
        filler_count = 0
        neg_count = 0
        for a in answers:
            # strip punctuation for counting
            toks = re.findall(r"\w+", a.lower())
            wc = len(toks)
            total_words += wc
            if wc <= 6:
                short += 1
            # filler and negative counts
            for f in filler_words:
                filler_count += a.lower().count(f)
            for n in negative_phrases:
                neg_count += a.lower().count(n)

        avg_len = float(total_words) / max(1, len(answers))
        filler_per_100 = (filler_count * 100.0) / max(1, total_words)
        short_ratio = float(short) / max(1, len(answers))
        return {
            "avg_answer_len_words": round(avg_len, 2),
            "short_answer_ratio": round(short_ratio, 3),
            "filler_per_100_words": round(filler_per_100, 2),
            "negative_phrase_count": float(neg_count),
        }

    def _derive_overall_score(
        self,
        hr_data: Dict[str, Any] | None,
        job_fit: Dict[str, Any] | None,
        hiring_decision: Dict[str, Any] | None,
        transcript_metrics: Dict[str, float],
    ) -> float:
        """Combine multiple signals into a robust 0-100 overall score with penalties.
        Strategy:
        - Base = weighted avg of HR criteria mean, job_fit.overall_fit_score*100, skill_match mean*100.
        - Penalties for poor transcript metrics (short answers, negative phrases, high filler).
        - Penalties for unmet high-importance requirements.
        - Cap score when hiring_decision recommends 'No Hire' or confidence low.
        """
        # Base components
        hr_scores: List[float] = []
        if isinstance(hr_data, dict):
            for c in hr_data.get("criteria", []) or []:
                try:
                    hr_scores.append(float(c.get("score_0_100", 0.0)))
                except Exception:
                    continue
        hr_mean = sum(hr_scores)/len(hr_scores) if hr_scores else 50.0

        jf = job_fit or {}
        job_fit_score = float(jf.get("overall_fit_score", 0.5)) * 100.0

        hd = hiring_decision or {}
        sm = (hd.get("skill_match") or {}) if isinstance(hd, dict) else {}
        try:
            skill_vals = [float(sm.get(k, 0.5)) * 100.0 for k in ("technical_fit","soft_skills_fit","cultural_fit","growth_potential")]
            skill_mean = sum(skill_vals)/len(skill_vals)
        except Exception:
            skill_mean = 50.0

        # Weighted base
        base = 0.4*hr_mean + 0.4*job_fit_score + 0.2*skill_mean

        # Requirement penalties
        reqs = jf.get("requirements_matrix") if isinstance(jf, dict) else []
        high_missing = 0
        partial_high = 0
        if isinstance(reqs, list):
            for r in reqs:
                try:
                    importance = str(r.get("importance", "medium")).lower()
                    meets = str(r.get("meets", "neither")).lower()
                    if importance == "high":
                        if meets in ("no", "neither"):
                            high_missing += 1
                        elif meets == "partial":
                            partial_high += 1
                except Exception:
                    continue
        penalty = 0.0
        penalty += high_missing * 12.0  # strong penalty per high-importance miss
        penalty += partial_high * 6.0

        # Transcript penalties
        short_ratio = transcript_metrics.get("short_answer_ratio", 0.0)
        filler_per_100 = transcript_metrics.get("filler_per_100_words", 0.0)
        neg_count = transcript_metrics.get("negative_phrase_count", 0.0)
        if short_ratio >= 0.5:
            penalty += 12.0
        elif short_ratio >= 0.3:
            penalty += 6.0
        if filler_per_100 >= 8.0:
            penalty += 8.0
        elif filler_per_100 >= 4.0:
            penalty += 4.0
        if neg_count >= 6:
            penalty += 12.0
        elif neg_count >= 3:
            penalty += 6.0

        # Hiring decision gating
        rec = str(hd.get("hire_recommendation", "")).lower() if isinstance(hd, dict) else ""
        conf = float(hd.get("decision_confidence", 0.5)) if isinstance(hd, dict) else 0.5
        if rec == "no hire":
            base = min(base, 49.0)
        elif rec == "hold" and conf >= 0.6:
            base = min(base, 59.0)

        final_score = max(0.0, min(100.0, round(base - penalty, 2)))
        return final_score

    # --- Rubric mapping (role-based weights) ---
    def _infer_rubric_weights(self, job_title: str) -> Dict[str, float]:
        jt = (job_title or "").lower()
        # keys: problem, technical, communication, culture
        if any(k in jt for k in ["developer", "yazÄ±lÄ±m", "engineer", "mÃ¼hendis"]):
            return {"problem": 0.25, "technical": 0.45, "communication": 0.15, "culture": 0.15}
        if any(k in jt for k in ["data", "ml", "ai", "bilim"]):
            return {"problem": 0.25, "technical": 0.45, "communication": 0.20, "culture": 0.10}
        if any(k in jt for k in ["product", "Ã¼rÃ¼n", "pm"]):
            return {"problem": 0.25, "technical": 0.25, "communication": 0.25, "culture": 0.25}
        if any(k in jt for k in ["sales", "satÄ±ÅŸ", "bdm", "business development"]):
            return {"problem": 0.15, "technical": 0.20, "communication": 0.35, "culture": 0.30}
        if any(k in jt for k in ["marketing", "pazarlama", "growth"]):
            return {"problem": 0.20, "technical": 0.25, "communication": 0.35, "culture": 0.20}
        if any(k in jt for k in ["finance", "muhasebe", "denetim", "finans"]):
            return {"problem": 0.30, "technical": 0.35, "communication": 0.20, "culture": 0.15}
        if any(k in jt for k in ["support", "destek", "mÃ¼ÅŸteri"]):
            return {"problem": 0.15, "technical": 0.20, "communication": 0.40, "culture": 0.25}
        if any(k in jt for k in ["cto", "cmo", "ceo", "cfo", "vp", "director", "mÃ¼dÃ¼r", "yÃ¶netici", "executive"]):
            return {"problem": 0.30, "technical": 0.20, "communication": 0.25, "culture": 0.25}
        return {"problem": 0.25, "technical": 0.35, "communication": 0.20, "culture": 0.20}

    def _compute_rubric(self, job_title: str, hr_data: Dict[str, Any] | None, job_fit: Dict[str, Any] | None, hiring_decision: Dict[str, Any] | None) -> Dict[str, Any]:
        weights = self._infer_rubric_weights(job_title)
        # Map sources to rubric criteria
        # problem -> HR "Problem Ã‡Ã¶zme"
        # technical -> avg(job_fit.overall_fit_score*100, hiring_decision.skill_match.technical_fit*100)
        # communication -> HR "Ä°letiÅŸim NetliÄŸi"
        # culture -> avg(HR "TakÄ±m Ã‡alÄ±ÅŸmasÄ±", hiring_decision.skill_match.cultural_fit*100)
        def _hr(label: str) -> float | None:
            try:
                for c in (hr_data or {}).get("criteria", []) or []:
                    if str(c.get("label", "")).lower().startswith(label.lower()[:5]):
                        return float(c.get("score_0_100", 0.0))
            except Exception:
                return None
            return None
        problem = _hr("Problem") or 50.0
        comm = _hr("Ä°letiÅŸim") or 50.0
        team = _hr("TakÄ±m") or 50.0
        jf_score = float((job_fit or {}).get("overall_fit_score", 0.5)) * 100.0
        sm = (hiring_decision or {}).get("skill_match", {}) if isinstance(hiring_decision, dict) else {}
        tech_match = float(sm.get("technical_fit", 0.5)) * 100.0
        cult_match = float(sm.get("cultural_fit", 0.5)) * 100.0
        technical = (jf_score + tech_match) / 2.0
        culture = (team + cult_match) / 2.0
        criteria = [
            {"label": "Problem Ã‡Ã¶zme", "score_0_100": round(problem, 2), "weight": weights["problem"]},
            {"label": "Teknik Yeterlilik", "score_0_100": round(technical, 2), "weight": weights["technical"]},
            {"label": "Ä°letiÅŸim", "score_0_100": round(comm, 2), "weight": weights["communication"]},
            {"label": "KÃ¼ltÃ¼r/Ä°ÅŸ UygunluÄŸu", "score_0_100": round(culture, 2), "weight": weights["culture"]},
        ]
        overall = 0.0
        for c in criteria:
            overall += float(c["score_0_100"]) * float(c["weight"])
        return {"criteria": criteria, "overall": round(overall, 2), "weights": weights}
    
    def _create_hr_criteria_prompt(self, transcript: str) -> str:
        """Create HR criteria analysis prompt"""
        return f"""Sen deneyimli bir HR uzmanÄ±sÄ±n. AÅŸaÄŸÄ±daki mÃ¼lakat transkriptini analiz et ve her kriter iÃ§in objektif, kanÄ±ta dayalÄ± deÄŸerlendirme yap.

DEÄžERLENDIRME KRÄ°TERLERÄ°:
1. Ä°letiÅŸim NetliÄŸi (0-100): AÃ§Ä±k ifade, yapÄ±landÄ±rÄ±lmÄ±ÅŸ cevaplar, dinleme becerisi
2. Problem Ã‡Ã¶zme (0-100): Analitik dÃ¼ÅŸÃ¼nme, Ã§Ã¶zÃ¼m odaklÄ±lÄ±k, yaratÄ±cÄ±lÄ±k
3. TakÄ±m Ã‡alÄ±ÅŸmasÄ± (0-100): Birlikte Ã§alÄ±ÅŸma Ã¶rnekleri, iÅŸbirliÄŸi, Ã§atÄ±ÅŸma yÃ¶netimi
4. Liderlik (0-100): Ä°nisiyatif alma, yÃ¶nlendirme, sorumluluk Ã¼stlenme
5. BÃ¼yÃ¼me Zihniyeti (0-100): Ã–ÄŸrenme isteÄŸi, hatalardan ders alma, geliÅŸim odaklÄ±lÄ±k

PUANLAMA REHBERÄ°:
- 90-100: MÃ¼kemmel, Ã§ok gÃ¼Ã§lÃ¼ kanÄ±tlar
- 80-89: GÃ¼Ã§lÃ¼, net pozitif Ã¶rnekler
- 70-79: Ä°yi, bazÄ± pozitif gÃ¶stergeler
- 60-69: Orta, sÄ±nÄ±rlÄ± kanÄ±t
- 50-59: ZayÄ±f, minimal kanÄ±t
- 0-49: Yetersiz, kanÄ±t yok veya negatif

ZORUNLU JSON FORMAT:
{{
  "criteria": [
    {{
      "label": "Ä°letiÅŸim NetliÄŸi",
      "score_0_100": 85,
      "evidence": "Sorulara yapÄ±landÄ±rÄ±lmÄ±ÅŸ ve net cevaplar verdi.",
      "confidence": 0.9,
      "reasoning": "3 farklÄ± Ã¶rnekte net aÃ§Ä±klama ve somut detaylar saÄŸladÄ±."
    }}
  ],
  "summary": "Genel HR deÄŸerlendirme Ã¶zeti",
  "overall_score": 78.5,
  "meta": {{
    "total_response_time": "18 dakika",
    "answer_depth": "orta",
    "evidence_quality": "gÃ¼Ã§lÃ¼"
  }}
}}

MÃœLAKAT TRANSKRÄ°PTÄ°:
{transcript[:6000]}"""
    
    def _create_job_fit_prompt(self, job_desc: str, transcript: str, resume: str) -> str:
        """Create job fit analysis prompt"""
        return f"""Sen senior bir iÅŸe alÄ±m uzmanÄ±sÄ±n. Ä°ÅŸ tanÄ±mÄ±, Ã¶zgeÃ§miÅŸ ve mÃ¼lakat transkriptini detaylÄ± analiz et.

GÃ–REV: Her iÅŸ gereksinimini adayÄ±n profiliyle eÅŸleÅŸtir ve kanÄ±t seviyesini deÄŸerlendir.

KAYNAK TIPLERI:
- cv: Ã–zgeÃ§miÅŸte yazÄ±lÄ±/belgelenmiÅŸ
- interview: MÃ¼lakatta sÃ¶zlÃ¼ olarak kanÄ±tlanmÄ±ÅŸ  
- both: Hem Ã¶zgeÃ§miÅŸte hem mÃ¼lakatta teyit edilmiÅŸ
- neither: HiÃ§birinde kanÄ±t yok

KARÅžILAMA SEVÄ°YELERÄ°:
- yes: Tam olarak karÅŸÄ±lÄ±yor (gÃ¼Ã§lÃ¼ kanÄ±t)
- partial: KÄ±smen karÅŸÄ±lÄ±yor (sÄ±nÄ±rlÄ± kanÄ±t)
- no: KarÅŸÄ±lamÄ±yor (kanÄ±t yok)

ZORUNLU JSON FORMAT:
{{
  "job_fit_summary": "3-4 cÃ¼mlelik genel deÄŸerlendirme",
  "overall_fit_score": 0.75,
  "cv_existing_skills": ["Ã–zgeÃ§miÅŸte net olan yetenekler"],
  "interview_demonstrated": ["MÃ¼lakatta kanÄ±tlanan yetenekler"],
  "clear_gaps": ["AÃ§Ä±k eksiklik gÃ¶steren alanlar"],
  "requirements_matrix": [
    {{
      "label": "Spesifik yetenek/gereksinim",
      "meets": "yes|partial|no",
      "source": "cv|interview|both|neither",
      "evidence": "Somut kanÄ±t/Ã¶rnek",
      "confidence": 0.9,
      "importance": "high|medium|low"
    }}
  ],
  "recommendations": ["Spesifik iÅŸe alÄ±m Ã¶nerisi"],
  "risk_factors": ["Potansiyel risk alanlarÄ±"],
  "competitive_advantages": ["AdayÄ±n Ã¶ne Ã§Ä±kan artÄ±larÄ±"]
}}

Ä°Åž TANIMI:
{job_desc[:4500]}

Ã–ZGEÃ‡MIÅž:
{(resume or 'Ã–zgeÃ§miÅŸ bilgisi mevcut deÄŸil')[:3500]}

MÃœLAKAT TRANSKRÄ°PTÄ°:
{transcript[:4500]}"""
    
    def _create_hiring_decision_prompt(self, job_desc: str, transcript: str, resume: str) -> str:
        """Create hiring decision analysis prompt"""
        return f"""Sen deneyimli bir CTO ve hiring manager'sÄ±n. Ä°ÅŸ tanÄ±mÄ±, Ã¶zgeÃ§miÅŸ ve mÃ¼lakat transkriptine gÃ¶re yapÄ±landÄ±rÄ±lmÄ±ÅŸ iÅŸe alÄ±m kararÄ± ver.

GÃ–REV: Objektif, veri-destekli ve uygulama-odaklÄ± karar analizi yap.

DEÄžERLENDIRME Ã‡ERÃ‡EVESÄ°:
1. Teknik yeterlilik (iÅŸ gereksinimleri vs aday profili)
2. YumuÅŸak beceriler (takÄ±m uyumu, iletiÅŸim, liderlik)
3. BÃ¼yÃ¼me potansiyeli (Ã¶ÄŸrenme hÄ±zÄ±, adaptasyon)
4. KÃ¼ltÃ¼rel uyum (ÅŸirket deÄŸerleri, Ã§alÄ±ÅŸma tarzÄ±)
5. Risk faktÃ¶rleri (kÄ±rmÄ±zÄ± bayraklar, endiÅŸe alanlarÄ±)

HÄ°RE RECOMMENDATÄ°ON SEVÄ°YELERÄ°:
- Strong Hire: Kesinlikle iÅŸe alÄ±nmalÄ±, role mÃ¼kemmel uyum
- Hire: Ä°ÅŸe alÄ±nmalÄ±, gereksinimleri karÅŸÄ±lÄ±yor
- Hold: KararsÄ±z, ek bilgi/mÃ¼lakat gerekli
- No Hire: Ä°ÅŸe alÄ±nmamalÄ±, Ã¶nemli eksiklikler var

ZORUNLU JSON FORMAT:
{{
  "hire_recommendation": "Strong Hire|Hire|Hold|No Hire",
  "overall_assessment": "4-5 cÃ¼mlelik yapÄ±landÄ±rÄ±lmÄ±ÅŸ genel deÄŸerlendirme",
  "decision_confidence": 0.85,
  "key_strengths": ["Spesifik gÃ¼Ã§lÃ¼ yÃ¶n 1 (kanÄ±tla)"],
  "key_concerns": ["Spesifik endiÅŸe 1 (gerekÃ§eyle)"],
  "skill_match": {{
    "technical_fit": 0.8,
    "soft_skills_fit": 0.7,
    "cultural_fit": 0.9,
    "growth_potential": 0.8
  }},
  "salary_analysis": {{
    "candidate_expectation": "AdayÄ±n maaÅŸ beklentisi (varsa)",
    "market_alignment": "market_appropriate|too_high|too_low|belirtilmedi",
    "recommended_range": "Ã–nerilen maaÅŸ aralÄ±ÄŸÄ±",
    "negotiation_notes": "MaaÅŸ mÃ¼zakeresi stratejisi"
  }},
  "risk_factors": ["Potansiyel risk 1"],
  "mitigation_strategies": ["Risk azaltma Ã¶nerisi"],
  "next_steps": ["Ã–nerilen sonraki adÄ±m"],
  "timeline_recommendation": "immediate|1_week|2_weeks|reassess"
}}

Ä°Åž TANIMI:
{job_desc[:4000]}

Ã–ZGEÃ‡MIÅž:
{(resume or 'Ã–zgeÃ§miÅŸ bilgisi mevcut deÄŸil')[:2500]}

MÃœLAKAT TRANSKRÄ°PTÄ°:
{transcript[:4500]}"""
    
    def _create_candidate_profile_prompt(self, resume: str, job_desc: str) -> str:
        """Create candidate profile summary prompt"""
        return f"""AÅŸaÄŸÄ±daki Ã¶zgeÃ§miÅŸi analiz et ve iÅŸ ilanÄ±na gÃ¶re Ã¶z bir deÄŸerlendirme yap. 
Maksimum 4-5 cÃ¼mle ile ÅŸu bilgileri ver:

ðŸ“‹ **Profil Ã–zeti**: Deneyim seviyesi, ana uzmanlÄ±k alanlarÄ± ve eÄŸitim durumu
ðŸŽ¯ **Ä°ÅŸ UygunluÄŸu**: Ä°lan gereksinimlerine uygunluk ve Ã¶ne Ã§Ä±kan yetenekler
ðŸš€ **Ã–ne Ã‡Ä±kan BaÅŸarÄ±lar**: En dikkat Ã§ekici proje/deneyim (varsa)
ðŸ’¡ **Genel DeÄŸerlendirme**: KÄ±sa bir iÅŸe alÄ±m Ã¶nerisi

TÃ¼rkÃ§e ve profesyonel bir dil kullan. Ã‡ok uzun olmasÄ±n, Ã¶z ve net ol.

Ä°ÅŸ Ä°lanÄ±: {job_desc[:3000] or 'BelirtilmemiÅŸ'}

Ã–zgeÃ§miÅŸ:
{resume[:4000]}"""
    
    def _create_soft_skills_prompt(self, transcript: str, job_desc: str) -> str:
        """Create soft skills extraction prompt"""
        return f"""AÅŸaÄŸÄ±daki yanÄ±ttan soft-skills Ã§Ä±kar ve kÄ±sa Ã¶zet ver. 
YanÄ±t TÃ¼rkÃ§e olabilir. JSON dÃ¶n: {{"soft_skills":[{{"label":str,"confidence":0-1,"evidence":str}}],"summary":str}}.

Ä°ÅŸ tanÄ±mÄ±: {job_desc[:2000] or '-'}

YanÄ±t: {transcript[:4000]}"""
    
    def _create_requirements_extraction_prompt(self, job_desc: str) -> str:
        """Create requirements extraction prompt"""
        return f"""AÅŸaÄŸÄ±daki iÅŸ ilanÄ±ndan gereksinimleri Ã§Ä±kar ve normalize et. JSON dÃ¶n.
Åžema: {{"items":[{{"id":str,"label":str,"must":bool,"level":"junior|mid|senior|lead","weight":0-1,"keywords":[str],"success_rubric":str,"question_templates":[str]}}]}}

Ä°lan Metni:
{job_desc[:5000]}"""
    
    async def _run_single_analysis(self, analysis_type: AnalysisType, input_data: AnalysisInput) -> Tuple[AnalysisType, Union[Dict[str, Any], str]]:
        """Run single analysis type"""
        try:
            if analysis_type == AnalysisType.HR_CRITERIA:
                if not input_data.transcript_text.strip():
                    return analysis_type, {}
                
                prompt = self._create_hr_criteria_prompt(input_data.transcript_text)
                result = await generate_json(prompt, temperature=0.05)
                
                # Normalize result
                if not isinstance(result.get("criteria"), list):
                    result["criteria"] = []
                
                for criterion in result.get("criteria", []):
                    criterion.setdefault("confidence", 0.5)
                    criterion.setdefault("reasoning", "")
                
                return analysis_type, result
            
            elif analysis_type == AnalysisType.JOB_FIT:
                if not (input_data.job_description.strip() and input_data.transcript_text.strip()):
                    return analysis_type, {}
                
                prompt = self._create_job_fit_prompt(
                    input_data.job_description,
                    input_data.transcript_text,
                    input_data.resume_text
                )
                result = await generate_json(prompt, temperature=0.1)
                
                # Normalize result
                if not isinstance(result.get("requirements_matrix"), list):
                    result["requirements_matrix"] = []
                
                for req in result.get("requirements_matrix", []):
                    req.setdefault("confidence", 0.5)
                    req.setdefault("importance", "medium")
                
                result.setdefault("overall_fit_score", 0.0)
                return analysis_type, result
            
            elif analysis_type == AnalysisType.HIRING_DECISION:
                if not (input_data.job_description.strip() and input_data.transcript_text.strip()):
                    return analysis_type, {}
                
                prompt = self._create_hiring_decision_prompt(
                    input_data.job_description,
                    input_data.transcript_text,
                    input_data.resume_text
                )
                result = await generate_json(prompt, temperature=0.05)
                
                # Normalize result
                result.setdefault("decision_confidence", 0.5)
                result.setdefault("timeline_recommendation", "reassess")
                
                skill_match = result.setdefault("skill_match", {})
                skill_match.setdefault("technical_fit", 0.5)
                skill_match.setdefault("soft_skills_fit", 0.5)
                skill_match.setdefault("cultural_fit", 0.5)
                skill_match.setdefault("growth_potential", 0.5)
                
                return analysis_type, result
            
            elif analysis_type == AnalysisType.CANDIDATE_PROFILE:
                if not input_data.resume_text.strip():
                    return analysis_type, {"profile": ""}
                
                prompt = self._create_candidate_profile_prompt(
                    input_data.resume_text,
                    input_data.job_description
                )
                result = await self.llm_client.generate(LLMRequest(
                    prompt=prompt,
                    temperature=0.3,
                    max_tokens=500
                ))
                
                # Clean up formatting
                import re
                text = result.content.strip()
                text = re.sub(r"\n{3,}", "\n\n", text)
                
                return analysis_type, {"profile": text}
            
            elif analysis_type == AnalysisType.SOFT_SKILLS:
                if not input_data.transcript_text.strip():
                    return analysis_type, {}
                
                prompt = self._create_soft_skills_prompt(
                    input_data.transcript_text,
                    input_data.job_description
                )
                result = await generate_json(prompt, temperature=0.2)
                return analysis_type, result
            
            elif analysis_type == AnalysisType.REQUIREMENTS_EXTRACTION:
                if not input_data.job_description.strip():
                    return analysis_type, {"items": []}
                
                prompt = self._create_requirements_extraction_prompt(input_data.job_description)
                result = await generate_json(prompt, temperature=0.1)
                
                # Defensive normalization
                items = result.get("items") if isinstance(result, dict) else None
                if not isinstance(items, list):
                    return analysis_type, {"items": []}
                
                for i, item in enumerate(items):
                    if not isinstance(item, dict):
                        continue
                    item.setdefault("id", f"req_{i}")
                    item["weight"] = float(item.get("weight", 0.5) or 0.5)
                    
                    kws = item.get("keywords") or []
                    if not isinstance(kws, list) or not kws:
                        item["keywords"] = [item.get("label", "")]
                    
                    templates = item.get("question_templates") or []
                    if not isinstance(templates, list) or not templates:
                        item["question_templates"] = [f"{item.get('label','')} ile ilgili somut bir Ã¶rnek anlatÄ±r mÄ±sÄ±nÄ±z?"]
                    # Ensure success_rubric exists for UI/export clarity
                    if not isinstance(item.get("success_rubric"), str) or not item.get("success_rubric"): 
                        item["success_rubric"] = "Somut Ã¶rnek, sizin aksiyonlarÄ±nÄ±z ve Ã¶lÃ§Ã¼lebilir sonuÃ§ iÃ§ermeli."
                
                return analysis_type, {"items": items}
            
            else:
                return analysis_type, {}
        
        except Exception as e:
            # Log error but don't fail entire batch
            return analysis_type, {"error": str(e)}
    
    async def analyze_comprehensive(self, input_data: AnalysisInput) -> Dict[str, Any]:
        """
        Run comprehensive analysis with multiple types in parallel
        Replaces multiple nlp.py functions with single efficient call
        """
        
        # Create tasks for parallel execution
        analysis_types = input_data.analysis_types or []
        tasks = [
            self._run_single_analysis(analysis_type, input_data)
            for analysis_type in analysis_types
        ]
        
        # Run all analyses in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Collect results
        analysis_results = {}
        for result in results:
            if isinstance(result, Exception):
                continue
            
            if isinstance(result, tuple) and len(result) == 2:
                try:
                    analysis_type, data = result
                    analysis_results[analysis_type.value] = data
                except (TypeError, ValueError, AttributeError):
                    # Handle unpacking errors gracefully
                    continue
        
        # Robust overall score combining HR, job-fit, hiring decision and transcript penalties
        hr_data = analysis_results.get(AnalysisType.HR_CRITERIA.value) if isinstance(analysis_results, dict) else None
        job_fit = analysis_results.get(AnalysisType.JOB_FIT.value) if isinstance(analysis_results, dict) else None
        hiring_decision = analysis_results.get(AnalysisType.HIRING_DECISION.value) if isinstance(analysis_results, dict) else None
        transcript_metrics = self._compute_transcript_metrics(input_data.transcript_text or "")
        overall_score = self._derive_overall_score(hr_data, job_fit, hiring_decision, transcript_metrics)
        
        # Add metadata
        # Rubric summary
        rubric = self._compute_rubric(input_data.job_title, hr_data, job_fit, hiring_decision)

        analysis_results["rubric"] = rubric
        # Seed initial topics for early coverage from extracted requirements
        try:
            req_items = ((analysis_results.get(AnalysisType.JOB_FIT.value) or {}).get("requirements_matrix") or []) if isinstance(analysis_results, dict) else []
            topics = [it.get("label", "") for it in req_items[:6] if isinstance(it, dict) and it.get("label")]
            if topics:
                analysis_results.setdefault("dialog_plan", {})
                analysis_results["dialog_plan"]["topics"] = topics
        except Exception:
            pass

        analysis_results["meta"] = {
            "analysis_types": [t.value for t in (input_data.analysis_types or [])],
            "overall_score": overall_score,
            "input_sizes": {
                "job_description": len(input_data.job_description),
                "transcript": len(input_data.transcript_text),
                "resume": len(input_data.resume_text)
            },
            "candidate_name": input_data.candidate_name,
            "job_title": input_data.job_title,
            "transcript_metrics": transcript_metrics,
        }
        
        return analysis_results


# Convenience functions that maintain compatibility with old nlp.py interface
async def comprehensive_interview_analysis(
    job_desc: str,
    transcript_text: str,
    resume_text: str = "",
    candidate_name: str = "",
    job_title: str = ""
) -> Dict[str, Any]:
    """
    Main comprehensive analysis function - replaces multiple nlp.py functions
    """
    analyzer = ComprehensiveAnalyzer()
    
    input_data = AnalysisInput(
        job_description=job_desc,
        transcript_text=transcript_text,
        resume_text=resume_text,
        candidate_name=candidate_name,
        job_title=job_title,
        analysis_types=[
            AnalysisType.HR_CRITERIA,
            AnalysisType.JOB_FIT,
            AnalysisType.HIRING_DECISION
        ]
    )
    
    return await analyzer.analyze_comprehensive(input_data)


async def extract_requirements_spec(job_desc: str) -> Dict[str, Any]:
    """Extract job requirements - maintains compatibility"""
    if not job_desc.strip():
        return {"items": []}
    
    analyzer = ComprehensiveAnalyzer()
    input_data = AnalysisInput(
        job_description=job_desc,
        analysis_types=[AnalysisType.REQUIREMENTS_EXTRACTION]
    )
    
    result = await analyzer.analyze_comprehensive(input_data)
    return result.get(AnalysisType.REQUIREMENTS_EXTRACTION.value, {"items": []})


async def summarize_candidate_profile(resume_text: str, job_desc: str = "") -> str:
    """Summarize candidate profile - maintains compatibility"""
    if not resume_text.strip():
        return ""
    
    analyzer = ComprehensiveAnalyzer()
    input_data = AnalysisInput(
        resume_text=resume_text,
        job_description=job_desc,
        analysis_types=[AnalysisType.CANDIDATE_PROFILE]
    )
    
    result = await analyzer.analyze_comprehensive(input_data)
    profile_data = result.get(AnalysisType.CANDIDATE_PROFILE.value, {})
    if isinstance(profile_data, dict):
        return profile_data.get("profile", "")
    return str(profile_data) if profile_data else ""


# Backward compatibility functions (deprecated but maintained)
async def assess_hr_criteria(transcript_text: str) -> Dict[str, Any]:
    """Deprecated: Use comprehensive_interview_analysis instead"""
    analyzer = ComprehensiveAnalyzer()
    input_data = AnalysisInput(
        transcript_text=transcript_text,
        analysis_types=[AnalysisType.HR_CRITERIA]
    )
    result = await analyzer.analyze_comprehensive(input_data)
    return result.get(AnalysisType.HR_CRITERIA.value, {})


async def assess_job_fit(job_desc: str, transcript_text: str, resume_text: str = "") -> Dict[str, Any]:
    """Deprecated: Use comprehensive_interview_analysis instead"""
    analyzer = ComprehensiveAnalyzer()
    input_data = AnalysisInput(
        job_description=job_desc,
        transcript_text=transcript_text,
        resume_text=resume_text,
        analysis_types=[AnalysisType.JOB_FIT]
    )
    result = await analyzer.analyze_comprehensive(input_data)
    return result.get(AnalysisType.JOB_FIT.value, {})


async def opinion_on_candidate(job_desc: str, transcript_text: str, resume_text: str = "") -> Dict[str, Any]:
    """Deprecated: Use comprehensive_interview_analysis instead"""
    analyzer = ComprehensiveAnalyzer()
    input_data = AnalysisInput(
        job_description=job_desc,
        transcript_text=transcript_text,
        resume_text=resume_text,
        analysis_types=[AnalysisType.HIRING_DECISION]
    )
    result = await analyzer.analyze_comprehensive(input_data)
    return result.get(AnalysisType.HIRING_DECISION.value, {})
